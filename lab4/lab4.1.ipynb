{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03135afc-7d00-481a-aa52-9320d7c508d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd25cc9-9a00-4c4d-88b3-248dc55c64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приклад 1: I doubt Tom could do that by himself.\tСумніваюся, що Том зміг би це зробити самостійно.\tCC-BY 2.0 (France) Attribution: tatoeba.org #7186243 (CK) & #7879957 (deniko)\n",
      "Приклад 2: I've been honest with you.\tЯ була чесна з тобою.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2359153 (CK) & #6969454 (deniko)\n",
      "Приклад 3: I have a red car.\tУ мене червона машина.\tCC-BY 2.0 (France) Attribution: tatoeba.org #3058028 (CK) & #5764541 (deniko)\n"
     ]
    }
   ],
   "source": [
    "# Завантаження даних\n",
    "with open('ukr.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "\n",
    "# Перевірка кількох випадкових рядків\n",
    "for i in range(3):\n",
    "    print(f\"Приклад {i + 1}: {random.choice(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "089730be-f13f-4db4-98a0-73519cb74631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I thought that was you.', '[s] Я думала, це була ти. [e]')\n",
      "('Are you getting paid for doing this?', '[s] Тобі платять за те, що ти це робиш? [e]')\n",
      "('We hope to arrive on time.', '[s] Ми сподіваємося приїхати вчасно. [e]')\n",
      "('Dreaming costs nothing.', '[s] Сни безкоштовні. [e]')\n",
      "('When will you take a bath?', '[s] Коли ти прийматимеш ванну? [e]')\n"
     ]
    }
   ],
   "source": [
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, ukr, _ = line.split('\\t')\n",
    "    ukr = '[s] ' + ukr + ' [e]'\n",
    "    text_pairs.append((eng, ukr))\n",
    "\n",
    "for t in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e004bca-c3ec-4e3b-b910-3ac10d72fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кількість пар: 50000\n",
      "Тренувальний набір: 35000\n",
      "Валідаційний набір: 7500\n",
      "Тестовий набір: 7500\n"
     ]
    }
   ],
   "source": [
    "# Перемішування і розбиття на набори\n",
    "random.shuffle(text_pairs)\n",
    "data_size = len(text_pairs)\n",
    "num_val = int(0.15 * data_size)\n",
    "num_train = data_size - 2 * num_val\n",
    "\n",
    "train_pairs = text_pairs[:num_train]\n",
    "val_pairs = text_pairs[num_train:num_train + num_val]\n",
    "test_pairs = text_pairs[num_train + num_val:]\n",
    "\n",
    "print(f\"\\nКількість пар: {data_size}\")\n",
    "print(f\"Тренувальний набір: {len(train_pairs)}\")\n",
    "print(f\"Валідаційний набір: {len(val_pairs)}\")\n",
    "print(f\"Тестовий набір: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0f5776-1d71-46ed-a6f9-9ef85d1413bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підготовка текстів\n",
    "strip_chars = string.punctuation.replace('[', '').replace(']', '')\n",
    "vocabulary_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Функція стандартизації українських текстів\n",
    "def standardize_ukr_text(text):\n",
    "    return tf.strings.regex_replace(tf.strings.lower(text), f\"[{re.escape(strip_chars)}]\", '')\n",
    "\n",
    "# Текстові векторизатори\n",
    "eng_vector = keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "ukr_vector = keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=standardize_ukr_text\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "eng_vector.adapt(train_eng_texts)\n",
    "ukr_vector.adapt(train_ukr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "915abea1-3ace-432c-aa31-fb59698245ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6472\n",
      "[np.str_('do'), np.str_('im'), np.str_('dont'), np.str_('he'), np.str_('mary'), np.str_('was'), np.str_('have'), np.str_('in'), np.str_('me'), np.str_('it')]\n",
      "15000\n",
      "[np.str_('на'), np.str_('Мері'), np.str_('ти'), np.str_('Тома'), np.str_('з'), np.str_('я'), np.str_('у'), np.str_('Це'), np.str_('У'), np.str_('в')]\n"
     ]
    }
   ],
   "source": [
    "for v in [eng_vector, ukr_vector]:\n",
    "    print(len(v.get_vocabulary()))\n",
    "    print(v.get_vocabulary()[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e75be7-3620-4eaa-8344-96116d49ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, ukr):\n",
    "    eng = eng_vector(eng)\n",
    "    ukr = ukr_vector(ukr)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ukr_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ukr_texts = list(ukr_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ukr_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e96fe01-1916-4be4-a3fc-e97bf070b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder inputs shape: (64, 20)\n",
      "decoder inputs shape: (64, 20)\n",
      "targets shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'encoder inputs shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'decoder inputs shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8f7df55-7360-4dc8-a94b-13e5709ad6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерація позиційного кодування\n",
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    position = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]  # (seq_len, 1)\n",
    "    div_term = tf.exp(tf.range(0, embed_dim, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / embed_dim))  # (embed_dim // 2)\n",
    "    \n",
    "    # Compute sine and cosine for even and odd indices\n",
    "    sinusoidal = tf.concat([tf.sin(position * div_term), tf.cos(position * div_term)], axis=-1)\n",
    "    \n",
    "    return sinusoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1e07e6b-2e29-47bc-aea8-7cd325c5d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оновлення класів TransformerEncoder та TransformerDecoder з додаванням позиційного кодування\n",
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, latent_dim):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        self.attention = MultiHeadAttention(num_heads, embed_dim)  # Assuming you have this implemented\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        pos_encoding = get_positional_encoding(seq_len, self.embed_dim)\n",
    "        inputs = inputs + pos_encoding  # Add positional encoding to the input\n",
    "        attn_output = self.attention(inputs, inputs)  # Example attention mechanism\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e8e6c51-bcd5-42ed-8ea3-6ce4ce90cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.attention1 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention2 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential([keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, enc_output, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        pos_encoding = get_positional_encoding(seq_len, inputs.shape[-1])\n",
    "        inputs = inputs + pos_encoding  # Add positional encoding to the input\n",
    "        attn_output1 = self.attention1(inputs, inputs)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output1)\n",
    "        attn_output2 = self.attention2(out1, enc_output)\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn_output2)\n",
    "        ffn_output = self.dense_proj(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm3(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb5a1dae-b709-403e-8632-33a75cd50d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "# Вхідні дані для енкодера\n",
    "encoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, num_heads, latent_dim)(x, training=True)\n",
    "\n",
    "# Вхідні дані для декодера\n",
    "decoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embed_dim)(decoder_inputs)\n",
    "\n",
    "# Підключення декодера до виходів енкодера\n",
    "x = TransformerDecoder(embed_dim, num_heads, latent_dim)(x, encoder_outputs, training=True)\n",
    "decoder_outputs = keras.layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Створення моделі\n",
    "transformer = keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee085c3e-4cb0-4d5e-8a79-0268252d0b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 772ms/step - accuracy: 0.7211 - loss: 2.5059 - val_accuracy: 0.7574 - val_loss: 1.6232\n",
      "Epoch 2/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 885ms/step - accuracy: 0.7621 - loss: 1.6413 - val_accuracy: 0.7746 - val_loss: 1.4878\n",
      "Epoch 3/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 822ms/step - accuracy: 0.7733 - loss: 1.5360 - val_accuracy: 0.7846 - val_loss: 1.3970\n",
      "Epoch 4/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 833ms/step - accuracy: 0.7811 - loss: 1.4475 - val_accuracy: 0.7903 - val_loss: 1.3375\n",
      "Epoch 5/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 858ms/step - accuracy: 0.7857 - loss: 1.3615 - val_accuracy: 0.7905 - val_loss: 1.2831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f774ec44a0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae1439bd-276c-4715-8ee6-b435f8fe0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input: I've got a map.\n",
      "--output: [s] Том ти не [e]\n",
      "--input: Let's quit.\n",
      "--output: [s] Том не [e]\n",
      "--input: Do you know who made it?\n",
      "--output: [s] Том ти не [e]\n"
     ]
    }
   ],
   "source": [
    "ukr_vocab = ukr_vector.get_vocabulary()\n",
    "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
    "max_decoded_sentence_length = 20  \n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    # Токенізація вхідного англійського речення\n",
    "    tokenized_input_sentence = eng_vector([input_sentence])\n",
    "    \n",
    "    # Початковий токен для декодування\n",
    "    decoded_sentence = \"[s]\" \n",
    "    \n",
    "    # Генерація перекладу з кількох кроків\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        # Токенізація поточного перекладеного речення\n",
    "        tokenized_target_sentence = ukr_vector([decoded_sentence])[:, :-1]\n",
    "        \n",
    "        # Прогнозування наступних токенів\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        \n",
    "        # Вибір найбільш ймовірного токена на поточному кроці\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])  \n",
    "        sampled_token = ukr_index_lookup[sampled_token_index]  \n",
    "        \n",
    "        decoded_sentence += \" \" + sampled_token \n",
    "        \n",
    "        # Якщо досягли кінцевого токена [e], припиняємо переклад\n",
    "        if sampled_token == \"[e]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n",
    "\n",
    "# Тестування перекладу для кількох речень\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "\n",
    "# Виведемо переклад для кількох речень\n",
    "for _ in range(3):\n",
    "    input_sentence = random.choice(test_eng_texts)  \n",
    "    translated = decode_sequence(input_sentence)  \n",
    "    print(f'--input: {input_sentence}')\n",
    "    print(f'--output: {translated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820230a1-3014-4a44-9bd3-4d5f605db938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
